{"cells":[{"metadata":{"_uuid":"1b1c16628c2f62a18e1dc2068e1d67d7003922b1"},"cell_type":"markdown","source":"# Introduction\n\n\n## Dataset\n\nFashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\n\n## Content\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.   \n\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.   \n\nThe training and test data sets have 785 columns.   \n\nThe first column consists of the class labels (see above), and represents the article of clothing. \n\nThe rest of 784 columns (1-785) contain the pixel-values of the associated image."},{"metadata":{"_uuid":"5a708dc52b2e5990e2247ed573d50d0f6933b730"},"cell_type":"markdown","source":"# Load packages"},{"metadata":{"_uuid":"2defa674e4e6d0e7371df92e7d1f388fc5c14bb6","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.python import keras\n\nfrom tensorflow.python.keras import optimizers\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization\nfrom tensorflow.python.keras. preprocessing.image import ImageDataGenerator\nfrom IPython.display import SVG\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e17fd2539412442ddb3ecacf84366ddd62faf836"},"cell_type":"markdown","source":"## Parameters"},{"metadata":{"_uuid":"d7a44bfc7a28df7026241c4a7e047298446f1554","trusted":true},"cell_type":"code","source":"IMG_ROWS = 28\nIMG_COLS = 28\nNUM_CLASSES = 10\nTEST_SIZE = 0.2\nRANDOM_STATE = 2018\n#Model\nNO_EPOCHS = 50\nBATCH_SIZE = 128\n\nIS_LOCAL = True\n\n","execution_count":142,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilities\n\nFunctions, kindly provided by people from Kaggle, to allow some easier description of models and plotting."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"\ndef get_classes_distribution(data):\n    # Get the count for each label\n    label_counts = data[\"label\"].value_counts()\n\n    # Get total number of samples\n    total_samples = len(data)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count / total_samples) * 100\n        print(\"{:<20s}:   {} or {}%\".format(label, count, percent))\n        \ndef sample_images_data(data):\n    # An empty list to collect some samples\n    sample_images = []\n    sample_labels = []\n\n    # Iterate over the keys of the labels dictionary defined in the above cell\n    for k in labels.keys():\n        # Get four samples for each category\n        samples = data[data[\"label\"] == k].head(4)\n        # Append the samples to the samples list\n        for j, s in enumerate(samples.values):\n            # First column contain labels, hence index should start from 1\n            img = np.array(samples.iloc[j, 1:]).reshape(IMG_ROWS,IMG_COLS)\n            sample_images.append(img)\n            sample_labels.append(samples.iloc[j, 0])\n\n    print(\"Total number of sample images to plot: \", len(sample_images))\n    return sample_images, sample_labels\n\ndef plot_images(data_index,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(4,4, figsize=(15,15))\n\n    for i, indx in enumerate(data_index[:16]):\n        ax[i//4, i%4].imshow(X_test[indx].reshape(IMG_ROWS,IMG_COLS), cmap=cmap)\n        ax[i//4, i%4].axis('off')\n        ax[i//4, i%4].set_title(\"True:{}  Pred:{}\".format(labels[y_true[indx]],labels[predicted_classes[indx]]))\n    plt.show()    \n    \n\ndef plot_sample_images(data_sample_images,data_sample_labels,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,8, figsize=(16,10))\n\n    for i, img in enumerate(data_sample_images):\n        ax[i//8, i%8].imshow(img, cmap=cmap)\n        ax[i//8, i%8].axis('off')\n        ax[i//8, i%8].set_title(labels[data_sample_labels[i]])\n    plt.show()    \n    \n\ndef create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['acc']\n    val_acc = hist['val_acc']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    \n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n   \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n\n    \n    iplot(fig, filename='accuracy-loss')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f49d65b4af77e87ab34b6854850619911cff1e6d"},"cell_type":"markdown","source":"# Read the data\n\nThere are 10 different classes of images, as following: \n\n* **0**: **T-shirt/top**;   \n* **1**: **Trouser**;   \n* **2**: **Pullover**;   \n* **3**: **Dress**;\n* **4**: **Coat**;\n* **5**: **Sandal**;\n* **6**: **Shirt**;\n* **7**: **Sneaker**;\n* **8**: **Bag**;\n* **9**: **Ankle boot**.\n\nImage dimmensions are **28**x**28**.   \n\nThe train set and test set are given in two separate datasets.\n"},{"metadata":{"_uuid":"a9c3148fda056ecc88570b302f5185064d5e9fc8","trusted":false},"cell_type":"code","source":"import os\n\nif(IS_LOCAL):\n    PATH=\"../input/\"\n    train_file = PATH+\"fashion-mnist_train.csv\"\n    test_file  = PATH+\"fashion-mnist_test.csv\"\n\n    train_data = pd.read_csv(train_file)\n    test_data = pd.read_csv(test_file)\nelse:\n    ((X, y), (X_test, y_test)) = fashion_mnist.load_data()\nprint(os.listdir(PATH))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"290f8f38b2f64dfd1bd3432eb1e9a101dbbaf4e5"},"cell_type":"markdown","source":"# Data analysis"},{"metadata":{"_uuid":"dc317fbf40c9b2838f8091e745a64e060a7affc0"},"cell_type":"markdown","source":"## Class distribution\n\nLet's see how many number of images are in each class. It's good to know this because an imbalanced dataset could lead to some trouble, and this should be known before designing the model."},{"metadata":{"_uuid":"da948eb587daf81160a4794a36662f8872c882a4","trusted":false},"cell_type":"code","source":"# Create a dictionary for each type of label \nlabels = {0 : \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n\n\nget_classes_distribution(train_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"588beb3c38b5f5a0b41ca0699426a1477efb8412"},"cell_type":"markdown","source":"## Sample images\n\nLet's plot some samples for the images to get a feeling of what the data looks like.  \nWe add labels to the train set images, with the corresponding fashion item category.  "},{"metadata":{"_uuid":"62fd41c12740f184ec160d692b3289d4457297d6","trusted":false},"cell_type":"code","source":"train_sample_images, train_sample_labels = sample_images_data(train_data)\nplot_sample_images(train_sample_images,train_sample_labels, \"Greens\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"273c79c7470c49fe7a546dd7148e0536be2dee19"},"cell_type":"markdown","source":"# Model"},{"metadata":{"_uuid":"10b920b2ffc5de4a3ddebe494966d90cca91ee95"},"cell_type":"markdown","source":"## Data preprocessing\n\nFirst we will do a data preprocessing to prepare for the model.\n\nWe reshape the columns from (784) to (28,28,1). We also save label (target) feature as a separate vector."},{"metadata":{"_uuid":"db4c308b9fb334a54b4c055b00d2a1fbcf77ab96","trusted":false},"cell_type":"code","source":"# data preprocessing\ndef data_preprocessing(raw):\n    out_y = keras.utils.to_categorical(raw.label, NUM_CLASSES)\n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:]\n    x_shaped_array = x_as_array.reshape(num_images, IMG_ROWS, IMG_COLS, 1)\n    out_x = x_shaped_array / 255\n    return out_x, out_y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d3cafa55173d40cd9a42df63d4919f03b264c09"},"cell_type":"markdown","source":"We process both the train_data and the test_data"},{"metadata":{"_uuid":"454d7a8fdca4bdbefc04a9d796de04b6af4a1767","trusted":false},"cell_type":"code","source":"# prepare the data\n# This is only needed if we are using local data\nif(IS_LOCAL):\n    X, y = data_preprocessing(train_data)\n    X_test, y_test = data_preprocessing(test_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e25c569f0a0e817ca0462f3e3ea2f50feb480b0"},"cell_type":"markdown","source":"## Split data in train and validation set\n\nWe further split the train set in train and validation set. A good rule of thumb is to take 20% of the data for the validation set."},{"metadata":{"_uuid":"36eb910dd0868a227a73c9759d6aee0a47ba2b1e","trusted":false},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e623ca7c0c61634c0e4b890fc5cf5cacb132552"},"cell_type":"markdown","source":"Now let's see the final shape of the processed data for training, validation and testing."},{"metadata":{"_uuid":"98b3f1bf07e8ed614bf32b8e1955cbe90bff21c5","trusted":false},"cell_type":"code","source":"print(\"Fashion MNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:4])\nprint(\"Fashion MNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:4])\nprint(\"Fashion MNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:4])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd9405b5a3fe0c7bd5bebe0616190a8cf26d2811"},"cell_type":"markdown","source":"## Train the model\n\n### Build the model   \n\n\nThe initial model will be a simple model good enough for small image recognition as it is needed. \nIt will be constructed from three 3x3x1 convolutional layers, with MaxPooling layers in between, and a fully connected layer at the end."},{"metadata":{"_uuid":"e31836cd5ec9d86340485404b8f613d1f574aca4","trusted":false},"cell_type":"code","source":"# Model\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_uniform',\n                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, \n                 kernel_size=(3, 3), \n                 activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu', kernel_initializer = 'he_uniform'))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\n\nn_opt = optimizers.SGD( momentum=0.9, nesterov=True)\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=n_opt,\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecb450d70539a62fb310bb7ed44849d2d01481ee"},"cell_type":"markdown","source":"### Inspect the model\n\nWe can get a summary and a plot of the model that was created."},{"metadata":{"_uuid":"a9863ccbdc1a3f9d03fc6f3dbbd3f2713995d03e","trusted":false},"cell_type":"code","source":"model.summary()\n\nplot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68a2e932241f29e52a3150b3fcf3fe9c21243be2"},"cell_type":"markdown","source":"### Train the model\n\nNow we can train the model with the already setup datasets for training and validation. The test set will be used later on during evaluation."},{"metadata":{"_kg_hide-output":true,"_uuid":"400494b7e0525069175625422e8c300bd7b41c51","scrolled":true,"trusted":false},"cell_type":"code","source":"train_model = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8699df44bc0a95f1a43a201d3dd566746d87173f"},"cell_type":"markdown","source":"## Test prediction accuracy\n\nAfter the model has been trained, we can evaluate it.Chosen type of evaluation for the process is accuracy."},{"metadata":{"_uuid":"b9e2bb7f25b02d491e34dd0d6d05943e287ae369","trusted":false},"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d779b0e47b8263370031c292a231b69decad374"},"cell_type":"markdown","source":"Test accuracy is  around  0.91.\n\nWe evaluated the model accuracy based on the predicted values for the test set.  Let's check the validation value during training.\n\n"},{"metadata":{"_uuid":"2b08e362a264d65687c49919f477e5d11c84d658"},"cell_type":"markdown","source":"## Validation accuracy and loss\n\nLet's plot the train and validation accuracy and loss, from the train history."},{"metadata":{"_uuid":"c034e2f70f0a26f453a764cd3dc6bbdebb3c4c53","trusted":false},"cell_type":"code","source":"plot_accuracy_and_loss(train_model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"678f32a0980699b8aa41a3f98840bcc1eabb2aa8"},"cell_type":"markdown","source":"The validation accuracy does not improve after few epochs and the validation loss is increasing after few epochs. This confirms our assumption that the model is overfitted. We will try to improve the model by adding Dropout layers."},{"metadata":{"_uuid":"5c026022d95d3afefc3e2cf47050c3db5f800ee1"},"cell_type":"markdown","source":"## Add Dropout layers to the model\n\nWe add several Dropout layers to the model, to help avoiding overfitting.    \nDropout is helping avoid overfitting in several ways, as explained in <a href='#refs'>[6]</a> and <a href='#refs'>[7]</a>.  \nSmaller dropout for Convolutional layers as suggested in:<a href='#refs'>[8]</a> "},{"metadata":{"_uuid":"6aff87cd33a993a60d701c35026ce8ea19d8665a","trusted":false},"cell_type":"code","source":"# Model\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\nmodel.add(MaxPooling2D((2, 2)))\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, \n                 kernel_size=(3, 3), \n                 activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\n# Add dropouts to the model\nmodel.add(Dropout(0.3))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\nn_opt = optimizers.SGD( momentum=0.9, nesterov=True)\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=n_opt,\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46811947e60224235b1ffaed48e7367b3ba68833"},"cell_type":"markdown","source":"## Model with dropout summary\n\nLet's inspect first the model."},{"metadata":{"_uuid":"594c8f00280db3fd6d4719791326d73c4769117d","trusted":false},"cell_type":"code","source":"model.summary()\n\nplot_model(model, to_file='model.png')\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99a964cc1478f1a8583e803ad7eeb35aadd15995"},"cell_type":"markdown","source":"Again let's run the new model."},{"metadata":{"_kg_hide-output":true,"_uuid":"83029bfb27f99fb6be2ecedc1bd256c0e861456c","trusted":false},"cell_type":"code","source":"train_model = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val))              ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f201d6925f70ad2420b3ee7e3a204cdc77f86723"},"cell_type":"markdown","source":"## Model with added droupout evaluation\n\nAfter adding the Dropout layers, the validation accuracy and validation loss are much better. Let's check now the prediction for the test set.\n\nLet's re-evaluate the test prediction accuracy with the new model."},{"metadata":{"_uuid":"d8e9293e99e34010bd456af7435c4cc7ccebdefd","trusted":false},"cell_type":"code","source":"plot_accuracy_and_loss(train_model)\n\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"572baa4ad0f12f490337137f7f9fb56a8a9a48c6"},"cell_type":"markdown","source":"Also the test accuracy improved. The test accuracy is now approximately 0.92."},{"metadata":{},"cell_type":"markdown","source":"# Further model optimizations"},{"metadata":{},"cell_type":"markdown","source":"We can try to build on this furhter. \n\nEven though  Adam optimization can cause problems <a href='#refs'>[9]</a> It might prove ok for our application. \n\nAfter each convolutional layer Batch normalization can be added. <a href='#refs'>[10]</a> <a href='#refs'>[11]</a>\n\nELU as an activation function instead of ReLU could bring an improvement as well. This will slightly slow down training, but the model will be faster to converge."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Model\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='elu',\n                 kernel_initializer='he_uniform',\n                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, \n                 kernel_size=(3, 3), \n                 activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='elu'))\nmodel.add(BatchNormalization())\n\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='elu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoints\n\nLet's add checkpoint saving as well. This will save the weights of the model which had the smallest validation loss, and should save the weights before the model starts overfitting."},{"metadata":{"trusted":false},"cell_type":"code","source":"filepath=\"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train_model = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val), callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reload the best weights and recompile the model (needed in order to make predictions)."},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_weights(\"weights.best.hdf5\")\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_accuracy_and_loss(train_model)\n\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data augmentation"},{"metadata":{},"cell_type":"markdown","source":"Finally we can try to add some data augmentation on the model. The data is safe to augment with horizontal flips."},{"metadata":{"trusted":false},"cell_type":"code","source":"image_aug = ImageDataGenerator(horizontal_flip = True)\nimage_aug.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's recompile the previous model and retrain it with augmented data."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"# Model\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='elu',\n                 kernel_initializer='he_uniform',\n                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2, 2)))\n\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, \n                 kernel_size=(3, 3), \n                 activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Add dropouts to the model\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), activation='elu'))\nmodel.add(BatchNormalization())\n\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='elu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"train_model = model.fit_generator(image_aug.flow(X_train, y_train, batch_size = BATCH_SIZE),\n                  steps_per_epoch=X_train.shape[0]/BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val), callbacks= callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_accuracy_and_loss(train_model)\n\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions of the trained model"},{"metadata":{},"cell_type":"markdown","source":"We can further test some metrics of the model when faced with predictions such as precision, recall, and the F1 score."},{"metadata":{"_uuid":"e519f82cc29b0dd8c5145d612da3eb496e2b321d","trusted":false},"cell_type":"code","source":"#get the predictions for the test data\npredicted_classes = model.predict_classes(X_test)\n#get the indices to be plotted\ny_true = test_data.iloc[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5989dd811570fe73b1f6b0715b06c428e578669","trusted":false},"cell_type":"code","source":"p = predicted_classes[:10000]\ny = y_true[:10000]\ncorrect = np.nonzero(p==y)[0]\nincorrect = np.nonzero(p!=y)[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46acb309f135a4082b4e1cd9b98a613b888f7c7e","trusted":false},"cell_type":"code","source":"print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3bfd05450aaee55c9e5edc7fb84d884c1d3e0b","trusted":false},"cell_type":"code","source":"target_names = [\"Class {} ({}) :\".format(i,labels[i]) for i in range(NUM_CLASSES)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"614afbc7ed760856cf9b47f879334902d1b6545b"},"cell_type":"markdown","source":"# Visualize classified images\n\n## Correctly classified images\n\n\nFinnally, let's visualize a few images to get a feeling at what the model does good and where it makes mistakes."},{"metadata":{"_uuid":"e64edf6e7ae3146eafe8da01f4dbc4e05c8bc1ce","trusted":false},"cell_type":"code","source":"plot_images(correct, \"Greens\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c7bb1b300a4530d2fa418eb56258a54587e3b90"},"cell_type":"markdown","source":"## Incorrectly classified images\n\nLet's see also few images incorrectly classified."},{"metadata":{"_uuid":"18b2843f6f8542cd95d0847617e84e959996341b","trusted":false},"cell_type":"code","source":"plot_images(incorrect, \"Reds\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e24053db4dbfec25fd1b90391586d7ce6bc32988"},"cell_type":"markdown","source":"# Conclusions\n\nWith a complex sequential model with multiple convolution layers and 50 epochs for the training, we obtained an accuracy ~0.91 for test prediction.\n\nHowever, on such a small dataset, a model like this is very prone to overfitting. An analysis of the validation accuracy and loss showed that this was true.\n\nAdding the dropout layers provided enough of regularization to prevent overfitting. This method slowed down training time, but it does not affect time needed for predictions, since the dropout layers are only used during training. Since this method provided an acuraccy of 92% it is a fairly good approach for some general applications.\n\nFurther optimizations can be done by adding Batch Normalization, changing the activation functions, using checkpoints and early stopping, but this all comes at a cost of longer training time, more parameters to train (which means more memory consumption), and more time needed for predictions.\n\nFinally, a good approach would be to add data augmentation to this set, such as horizontal flipping. Since data augmentation is done as a preprocessing step, this does not affect the performance time of neither training nor predictions, and can further increase accuracy."},{"metadata":{"_uuid":"1b45e3422fb9507da66ce7af67a97b54e7fa7683"},"cell_type":"markdown","source":"# <a id='refs'> References </a>\n\n[1] Fashion MNIST, An MNIST-like dataset of 70,000 28x28 labeled fashion images, https://www.kaggle.com/zalando-research/fashionmnist  \n[2] DanB, CollinMoris, Deep Learning From Scratch, https://www.kaggle.com/dansbecker/deep-learning-from-scratch  \n[3] DanB, Dropout and Strides for Larger Models, https://www.kaggle.com/dansbecker/dropout-and-strides-for-larger-models  \n[4] BGO, CNN with Keras, https://www.kaggle.com/bugraokcu/cnn-with-keras    \n[5] NAIN, EagerFMINST, https://www.kaggle.com/aakashnain/eagerfmnist  \n[6] Why Dropounts prevent overfitting in Deep Neural Networks, https://medium.com/@vivek.yadav/why-dropouts-prevent-overfitting-in-deep-neural-networks-937e2543a701  \n[7] Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf  \n[8] Vu Pham et al, Dropout improves Recurrent Neural Networks for Handwriting Recognition, https://arxiv.org/pdf/1312.4569.pdf\n[9] Ashia C. Wilson et al, The Marginal Value of Adaptive Gradient Methods in Machine Learning, https://arxiv.org/pdf/1705.08292.pdf \n[10] batch normalization, https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/\n[11] Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, https://arxiv.org/abs/1502.03167\n"},{"metadata":{},"cell_type":"markdown","source":"## Checkpoints\n\nLet's add checkpoint saving as well. This will save the weights of the model which had the smallest validation loss, and should save the weights before the model starts overfitting."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}